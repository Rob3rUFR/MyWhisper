# ğŸ¤ MyWhisper - Transcription Audio avec IA

Conteneur Docker plug-and-play pour la transcription audio utilisant **Faster Whisper Large v3 Turbo** et post-traitement via **Ollama**.

![Python](https://img.shields.io/badge/Python-3.11-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.110-green)
![Docker](https://img.shields.io/badge/Docker-Ready-blue)
![CUDA](https://img.shields.io/badge/CUDA-12.4-76B900)

## âœ¨ FonctionnalitÃ©s

### Transcription
- ğŸ¯ **Faster Whisper Large v3 Turbo** - Transcription rapide et prÃ©cise
- ğŸŒ **Multi-langue** - DÃ©tection automatique ou sÃ©lection manuelle (50+ langues)
- ğŸ“„ **Export multi-format** - JSON, TXT, SRT, VTT

### DictÃ©e en temps rÃ©el
- ğŸ™ï¸ **Enregistrement micro** - DictÃ©e vocale directement depuis le navigateur
- âš¡ **Transcription live** - RÃ©sultats en temps rÃ©el pendant l'enregistrement
- ğŸ”‡ **DÃ©tection silence** - ArrÃªt automatique aprÃ¨s 10s de silence
- ğŸ“Š **VU-mÃ¨tre** - Indicateur visuel du niveau audio

### Post-traitement IA (Ollama)
- ğŸ¤– **IntÃ©gration Ollama** - Connexion Ã  votre instance Ollama locale
- ğŸ“ **Prompts personnalisÃ©s** - CrÃ©ez vos propres prompts de reformulation
- ğŸ“‹ **Copie formatÃ©e** - Export optimisÃ© pour Word/Outlook avec mise en forme

### Interface & UX
- ğŸ–¥ï¸ **Interface web moderne** - Drag & drop, dark mode
- ğŸ“ˆ **Progression dÃ©taillÃ©e** - Suivi en temps rÃ©el de chaque Ã©tape
- ğŸ’¾ **Sauvegarde automatique** - Export direct vers un dossier de votre choix
- ğŸ”Œ **API OpenAI-compatible** - IntÃ©gration directe avec Open WebUI
- ğŸ”„ **Persistance de l'Ã©tat** - L'interface conserve son Ã©tat aprÃ¨s refresh
- ğŸ“‚ **Historique des transcriptions** - Gestion et tÃ©lÃ©chargement des anciennes transcriptions

### Performance
- ğŸš€ **GPU acceleration** - OptimisÃ© CUDA avec TF32
- ğŸ’ª **Support RTX 5090** - Compatible avec les derniers GPU NVIDIA
- âš¡ **VAD intÃ©grÃ©** - Filtrage automatique des silences
- ğŸ” **RÃ©cupÃ©ration auto** - Reprise aprÃ¨s perte de connexion

## ğŸš€ Quick Start

### PrÃ©requis

- Docker Desktop avec WSL2
- NVIDIA Container Toolkit ([Installation guide](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html))
- GPU CUDA-compatible (RTX 3000/4000/5000 series)
- (Optionnel) Ollama pour le post-traitement LLM

### Installation

```bash
# 1. Cloner le repo
git clone https://github.com/VOTRE_USERNAME/MyWhisper.git
cd MyWhisper

# 2. Configuration
copy env.example .env
# Ã‰diter .env si nÃ©cessaire

# 3. Build & Run
docker-compose up -d

# 4. AccÃ¨s
# Interface: http://localhost:8000
# API: http://localhost:8000/v1/audio/transcriptions
```

## ğŸ“– Utilisation

### Interface Web

AccÃ©der Ã  `http://localhost:8000` pour l'interface complÃ¨te.

#### Onglet Fichier
1. Glisser-dÃ©poser un fichier audio/vidÃ©o
2. SÃ©lectionner les options (langue, format)
3. Activer la sauvegarde automatique si souhaitÃ©
4. Cliquer sur "Transcrire"
5. Retraiter avec l'IA (Ollama) si configurÃ©

#### Onglet DictÃ©e
1. SÃ©lectionner la langue
2. Cliquer sur "DÃ©marrer"
3. Parler dans le micro
4. La transcription apparaÃ®t en temps rÃ©el
5. ArrÃªt automatique aprÃ¨s 10s de silence ou clic sur "ArrÃªter"

#### Onglet Historique
1. Voir toutes les transcriptions passÃ©es
2. TÃ©lÃ©charger dans diffÃ©rents formats (Texte, JSON, SRT, VTT)
3. Visualiser ou supprimer une transcription
4. Configurer la durÃ©e de rÃ©tention dans les paramÃ¨tres

#### Onglet ParamÃ¨tres
1. Configurer la durÃ©e de conservation de l'historique (1-365 jours)
2. Configurer l'URL Ollama (ex: `http://host.docker.internal:11434`)
3. SÃ©lectionner un modÃ¨le LLM
4. CrÃ©er des prompts personnalisÃ©s avec `{text}` comme placeholder

### API REST

#### Transcription (OpenAI-compatible)

```bash
curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F "file=@audio.mp3" \
  -F "language=fr" \
  -F "response_format=json"
```

#### RÃ©ponse JSON

```json
{
  "text": "Transcription complÃ¨te...",
  "language": "fr",
  "duration": 145.2,
  "segments": [
    {
      "id": 0,
      "start": 0.0,
      "end": 5.2,
      "text": "Bonjour, aujourd'hui..."
    }
  ]
}
```

#### Endpoints disponibles

| Endpoint | Description |
|----------|-------------|
| `GET /` | Interface web |
| `GET /health` | Health check (status GPU, modÃ¨les) |
| `GET /v1/models` | Liste des modÃ¨les disponibles |
| `POST /v1/audio/transcriptions` | Transcription OpenAI-compatible |
| `POST /v1/audio/transcriptions/stream` | Transcription avec SSE (Ã©vite timeout) |
| `POST /transcribe` | Endpoint simplifiÃ© |
| `GET /history` | Liste des transcriptions (pagination) |
| `GET /history/{id}` | DÃ©tails d'une transcription |
| `GET /history/{id}/download` | TÃ©lÃ©charger (format: text/json/srt/vtt) |
| `DELETE /history/{id}` | Supprimer une transcription |
| `GET /result/{client_id}` | RÃ©cupÃ©rer rÃ©sultat aprÃ¨s dÃ©connexion |

## âš™ï¸ Configuration Open WebUI

Pour utiliser MyWhisper comme moteur STT dans Open WebUI :

1. Aller dans **Settings > Audio > STT**
2. Configurer :
   - **Engine**: OpenAI
   - **Base URL**: `http://whisper-stt:8000/v1`
   - **API Key**: (laisser vide ou mettre une clÃ© factice)
   - **Model**: `whisper-large-v3`

### Configuration rÃ©seau Docker

Si Open WebUI est dans un autre conteneur, ajouter les deux au mÃªme rÃ©seau :

```yaml
# Dans docker-compose.yml de MyWhisper
networks:
  - openwebui_network

networks:
  openwebui_network:
    external: true
```

## ğŸ”§ Configuration avancÃ©e

### Variables d'environnement

| Variable | DÃ©faut | Description |
|----------|--------|-------------|
| `WHISPER_MODEL` | `large-v3-turbo` | ModÃ¨le Whisper Ã  utiliser |
| `DEVICE` | `cuda` | Device (cuda/cpu) |
| `COMPUTE_TYPE` | `float16` | Type de calcul (float16/int8) |
| `MAX_FILE_SIZE` | `524288000` | Taille max fichier (500MB) |
| `OLLAMA_URL` | - | URL de l'instance Ollama |
| `HISTORY_RETENTION_DAYS` | `90` | DurÃ©e conservation historique (jours) |

### ModÃ¨les Whisper disponibles

| ModÃ¨le | Vitesse | QualitÃ© | VRAM |
|--------|---------|---------|------|
| `large-v3` | RÃ©fÃ©rence | Meilleure | ~10GB |
| `large-v3-turbo` | ~2-3x plus rapide | LÃ©gÃ¨rement infÃ©rieure | ~6GB |
| `medium` | Rapide | Bonne | ~5GB |
| `small` | TrÃ¨s rapide | Correcte | ~2GB |

### Optimisation GPU

Pour RTX 4090/5090 (configuration optimale) :
```env
DEVICE=cuda
COMPUTE_TYPE=float16
```

Pour GPU avec moins de VRAM (8-12GB) :
```env
COMPUTE_TYPE=int8
```

## ğŸ“ Structure du projet

```
MyWhisper/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ env.example
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ PRD_Whisper_STT_Docker.md
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI app + routes
â”‚   â”œâ”€â”€ transcription.py     # Core STT logic (Faster Whisper)
â”‚   â”œâ”€â”€ history.py           # Gestion historique (SQLite)
â”‚   â”œâ”€â”€ patches.py           # Compatibility patches (PyTorch)
â”‚   â”œâ”€â”€ utils.py             # Helpers (formats, validation)
â”‚   â”œâ”€â”€ config.py            # Configuration (Pydantic)
â”‚   â”‚
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ index.html       # Interface web (4 onglets)
â”‚       â”œâ”€â”€ styles.css       # Dark theme moderne
â”‚       â””â”€â”€ app.js           # Alpine.js app
â”‚
â”œâ”€â”€ uploads/                 # Fichiers temporaires (volume)
â”œâ”€â”€ outputs/                 # Exports (volume)
â””â”€â”€ models/                  # ModÃ¨les tÃ©lÃ©chargÃ©s (volume, gitignore)
```

## ğŸ› Troubleshooting

### GPU non dÃ©tectÃ©

```bash
# VÃ©rifier NVIDIA Container Toolkit
docker run --rm --gpus all nvidia/cuda:12.4.1-runtime-ubuntu22.04 nvidia-smi
```

### Erreur CUDA out of memory

RÃ©duire la prÃ©cision dans `.env` :
```env
COMPUTE_TYPE=int8
```

Ou utiliser un modÃ¨le plus petit :
```env
WHISPER_MODEL=medium
```

### Erreur "weights_only" PyTorch

Le patch est automatiquement appliquÃ©. Si problÃ¨me, ajouter dans `docker-compose.yml` :
```yaml
environment:
  - TORCH_FORCE_WEIGHTS_ONLY_LOAD=0
```

### Port 8000 dÃ©jÃ  utilisÃ©

Changer le port dans `docker-compose.yml` :
```yaml
ports:
  - "8001:8000"
```

### Ollama non connectÃ©

1. VÃ©rifier qu'Ollama est lancÃ© : `ollama serve`
2. **Important** : Ollama doit Ã©couter sur `0.0.0.0` pour Ãªtre accessible depuis Docker
   - DÃ©finir `OLLAMA_HOST=0.0.0.0:11434` avant de lancer Ollama
3. Utiliser `http://host.docker.internal:11434` dans `.env`

## ğŸ“Š Performance

**Hardware testÃ©** : RTX 5090

| MÃ©trique | Valeur |
|----------|--------|
| Chargement modÃ¨le | ~5s |
| Transcription 1min audio | < 3s |
| Transcription 1h audio | ~5min |
| VRAM usage | ~4-6GB |

## ğŸ”„ Commandes utiles

```bash
# Build
docker-compose build

# DÃ©marrer
docker-compose up -d

# Logs en temps rÃ©el
docker-compose logs -f

# ArrÃªter
docker-compose down

# Reconstruire et relancer
docker-compose up -d --build

# VÃ©rifier GPU dans le conteneur
docker exec whisper-stt nvidia-smi

# Shell dans le conteneur
docker exec -it whisper-stt bash

# Supprimer cache modÃ¨les
docker exec whisper-stt rm -rf /app/models/*
docker-compose restart
```

## ğŸ†• Changelog

### v2.0.0 (Janvier 2026)
- âŒ **Suppression de la diarisation** - FonctionnalitÃ© retirÃ©e (non fiable)
- âœ… **ModÃ¨le large-v3-turbo** - Transcription 2-3x plus rapide
- âœ… **Interface simplifiÃ©e** - Plus d'options de diarisation
- âœ… **Image Docker allÃ©gÃ©e** - Suppression des dÃ©pendances NeMo

### v1.2.0 (Janvier 2026)
- âœ… **Historique des transcriptions** - Conservation avec durÃ©e configurable
- âœ… **Persistance de l'Ã©tat** - L'interface conserve son Ã©tat aprÃ¨s refresh
- âœ… **RÃ©cupÃ©ration automatique** - Reprise des rÃ©sultats aprÃ¨s perte de connexion

### v1.0.0
- âœ… Transcription Faster Whisper Large v3
- âœ… Interface web moderne (Alpine.js)
- âœ… API OpenAI-compatible
- âœ… DictÃ©e en temps rÃ©el avec dÃ©tection silence
- âœ… IntÃ©gration Ollama pour post-traitement LLM
- âœ… Support RTX 5090 (CUDA 12.4, PyTorch 2.9)

## ğŸ“œ Licence

MIT License

---

**CrÃ©Ã© avec â¤ï¸ pour une transcription audio simple et efficace**
