# ğŸ¤ MyWhisper - Transcription Audio avec IA

Conteneur Docker plug-and-play pour la transcription audio utilisant **Faster Whisper Large v3** avec diarisation des speakers via **Pyannote**.

![Python](https://img.shields.io/badge/Python-3.11-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.110-green)
![Docker](https://img.shields.io/badge/Docker-Ready-blue)
![CUDA](https://img.shields.io/badge/CUDA-12.1-76B900)

## âœ¨ FonctionnalitÃ©s

- âœ… **Faster Whisper Large v3** - Transcription rapide et prÃ©cise
- âœ… **Diarisation speakers** - Identification des intervenants (pyannote)
- âœ… **Multi-langue** - DÃ©tection automatique ou sÃ©lection manuelle
- âœ… **Export multi-format** - JSON, TXT, SRT, VTT
- âœ… **API OpenAI-compatible** - IntÃ©gration directe avec Open WebUI
- âœ… **Interface web** - Drag & drop moderne
- âœ… **GPU acceleration** - OptimisÃ© pour RTX 3090

## ğŸš€ Quick Start

### PrÃ©requis

- Docker Desktop avec WSL2
- NVIDIA Container Toolkit ([Installation guide](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html))
- GPU CUDA-compatible
- Token Hugging Face (pour la diarisation)

### Installation

```bash
# 1. Cloner le repo
git clone <repo_url>
cd whisper-stt

# 2. Configuration
copy env.example .env
# Ã‰diter .env et ajouter votre HF_TOKEN

# 3. Build & Run
docker-compose up -d

# 4. AccÃ¨s
# Interface: http://localhost:8000
# API: http://localhost:8000/v1/audio/transcriptions
```

### Obtenir un token Hugging Face

1. CrÃ©er un compte sur [huggingface.co](https://huggingface.co)
2. Aller dans Settings > Access Tokens
3. CrÃ©er un token avec accÃ¨s en lecture
4. **Important**: Accepter les conditions d'utilisation de [pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1)
5. Ajouter le token dans votre fichier `.env`

## ğŸ“– Utilisation

### Interface Web

AccÃ©der Ã  `http://localhost:8000` pour l'interface drag & drop.

1. Glisser-dÃ©poser un fichier audio
2. SÃ©lectionner les options (langue, format, diarisation)
3. Cliquer sur "Transcrire"
4. TÃ©lÃ©charger ou copier le rÃ©sultat

### API REST

#### Transcription (OpenAI-compatible)

```bash
curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F "file=@audio.mp3" \
  -F "language=fr" \
  -F "response_format=json" \
  -F "diarize=true"
```

#### RÃ©ponse JSON

```json
{
  "text": "Transcription complÃ¨te...",
  "language": "fr",
  "duration": 145.2,
  "segments": [
    {
      "id": 0,
      "start": 0.0,
      "end": 5.2,
      "text": "Bonjour, aujourd'hui...",
      "speaker": "SPEAKER_00"
    }
  ]
}
```

#### Autres endpoints

| Endpoint | Description |
|----------|-------------|
| `GET /` | Interface web |
| `GET /health` | Health check (status GPU, modÃ¨les) |
| `GET /v1/models` | Liste des modÃ¨les disponibles |
| `POST /v1/audio/transcriptions` | Transcription OpenAI-compatible |
| `POST /transcribe` | Endpoint simplifiÃ© |

## âš™ï¸ Configuration Open WebUI

Pour utiliser MyWhisper comme moteur STT dans Open WebUI :

1. Aller dans **Settings > Audio > STT**
2. Configurer :
   - **Engine**: OpenAI
   - **Base URL**: `http://whisper-stt:8000/v1`
   - **API Key**: (laisser vide ou mettre une clÃ© factice)
   - **Model**: `whisper-large-v3`

### Configuration rÃ©seau Docker

Si Open WebUI est dans un autre conteneur, ajouter les deux au mÃªme rÃ©seau :

```yaml
# Dans docker-compose.yml de MyWhisper
networks:
  - openwebui_network

networks:
  openwebui_network:
    external: true
```

## ğŸ”§ Configuration avancÃ©e

### Variables d'environnement

| Variable | DÃ©faut | Description |
|----------|--------|-------------|
| `WHISPER_MODEL` | `large-v3` | ModÃ¨le Whisper Ã  utiliser |
| `DEVICE` | `cuda` | Device (cuda/cpu) |
| `COMPUTE_TYPE` | `float16` | Type de calcul (float16/int8) |
| `HF_TOKEN` | - | Token Hugging Face (requis pour diarisation) |
| `ENABLE_DIARIZATION` | `true` | Activer la diarisation |
| `MAX_FILE_SIZE` | `524288000` | Taille max fichier (500MB) |

### Optimisation GPU

Pour RTX 3090 (configuration optimale) :
```env
DEVICE=cuda
COMPUTE_TYPE=float16
```

Pour GPU avec moins de VRAM :
```env
COMPUTE_TYPE=int8
```

## ğŸ“ Structure du projet

```
whisper-stt/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ env.example
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI app + routes
â”‚   â”œâ”€â”€ transcription.py     # Core STT logic
â”‚   â”œâ”€â”€ diarization.py       # Speaker diarization
â”‚   â”œâ”€â”€ utils.py             # Helpers
â”‚   â”œâ”€â”€ config.py            # Configuration
â”‚   â”‚
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ index.html       # Interface web
â”‚       â”œâ”€â”€ styles.css       # Styles
â”‚       â””â”€â”€ app.js           # Alpine.js app
â”‚
â”œâ”€â”€ uploads/                 # Fichiers temporaires (volume)
â”œâ”€â”€ outputs/                 # Exports (volume)
â””â”€â”€ models/                  # ModÃ¨les tÃ©lÃ©chargÃ©s (volume)
```

## ğŸ› Troubleshooting

### GPU non dÃ©tectÃ©

```bash
# VÃ©rifier NVIDIA Container Toolkit
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

### Erreur CUDA out of memory

RÃ©duire la prÃ©cision dans `.env` :
```env
COMPUTE_TYPE=int8
```

### Diarisation Ã©choue

1. VÃ©rifier que `HF_TOKEN` est dÃ©fini dans `.env`
2. Accepter les conditions sur [Hugging Face](https://huggingface.co/pyannote/speaker-diarization-3.1)
3. VÃ©rifier les logs : `docker-compose logs -f`

### Port 8000 dÃ©jÃ  utilisÃ©

Changer le port dans `docker-compose.yml` :
```yaml
ports:
  - "8001:8000"
```

## ğŸ“Š Performance

**Hardware cible** : RTX 3090 (24GB VRAM)

| MÃ©trique | Valeur |
|----------|--------|
| Chargement modÃ¨le | < 10s |
| Transcription 1min audio | < 5s |
| VRAM usage | < 6GB |
| Diarisation overhead | +30% temps max |

## ğŸ”„ Commandes utiles

```bash
# Build
docker-compose build

# DÃ©marrer
docker-compose up -d

# Logs en temps rÃ©el
docker-compose logs -f

# ArrÃªter
docker-compose down

# VÃ©rifier GPU dans le conteneur
docker exec whisper-stt nvidia-smi

# Shell dans le conteneur
docker exec -it whisper-stt bash

# Supprimer cache modÃ¨les
docker exec whisper-stt rm -rf /app/models/*
docker-compose restart
```

## ğŸ“œ Licence

MIT License

---

**CrÃ©Ã© avec â¤ï¸ pour une transcription audio simple et efficace**
